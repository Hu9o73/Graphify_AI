{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Construction Pipeline\n",
    "\n",
    "This notebook demonstrates the complete process of building a knowledge graph from raw text data. The pipeline includes:\n",
    "\n",
    "1. Text collection through web scraping\n",
    "2. Text preprocessing and cleaning\n",
    "3. Named Entity Recognition (NER)\n",
    "4. Relation Extraction (RE)\n",
    "5. Knowledge Graph Construction\n",
    "6. Visualization and querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for importing local modules\n",
    "# Adjust this path if needed\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data_collection.scraper import NewsArticleScraper\n",
    "from src.preprocessing.cleaner import clean_text\n",
    "from src.entity_recognition.ner import SpacyNERExtractor, CRFExtractor\n",
    "from src.entity_recognition.comparison import NERComparison\n",
    "from src.relation_extraction.extractor import SpacyRelationExtractor\n",
    "from src.knowledge_graph.builder import KnowledgeGraphBuilder\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('output/data', exist_ok=True)\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "os.makedirs('output/visualization', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Let's collect news articles from Reuters using our web scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "scraper = NewsArticleScraper(output_dir='output/data/raw')\n",
    "\n",
    "# Scrape articles\n",
    "# Note: This might take a while and might be rate-limited by the website\n",
    "# For demonstration, we can scrape a smaller number of articles\n",
    "# We can also use example data if scraping fails\n",
    "try:\n",
    "    article_files = scraper.scrape_reuters(num_articles=15, category='business')\n",
    "    print(f\"Scraped {len(article_files)} articles:\")\n",
    "    for file in article_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error scraping articles: {e}\")\n",
    "    # Use example data if scraping fails\n",
    "    logger.info(\"Using example data instead\")\n",
    "    # Create a simple example article\n",
    "    example_article = {\n",
    "        \"id\": \"example1\",\n",
    "        \"title\": \"Apple announces new partnership with Microsoft\",\n",
    "        \"url\": \"https://example.com/article1\",\n",
    "        \"source\": \"example\",\n",
    "        \"category\": \"business\",\n",
    "        \"published_date\": \"2023-01-01\",\n",
    "        \"scraped_date\": \"2023-01-02\",\n",
    "        \"content\": \"Apple Inc. has announced a new partnership with Microsoft Corporation, according to CEO Tim Cook. \\\n",
    "The collaboration will focus on cloud computing services and AI integration. \\\n",
    "The partnership was revealed at a press conference in Cupertino, California yesterday. \\\n",
    "Microsoft CEO Satya Nadella expressed excitement about working with the iPhone maker. \\\n",
    "Apple was founded by Steve Jobs in 1976 and has become one of the world's most valuable companies.\"\n",
    "    }\n",
    "    \n",
    "    # Save example article\n",
    "    os.makedirs('output/data/raw', exist_ok=True)\n",
    "    example_file = 'output/data/raw/example_article.json'\n",
    "    with open(example_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(example_article, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    article_files = [example_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Now let's preprocess the text from the articles we collected.\n",
    "We retrieve the .json files and check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles from raw data directory with duplicate detection\n",
    "import glob\n",
    "import hashlib\n",
    "\n",
    "def load_articles_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Load JSON articles from a directory with duplicate detection.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of unique article dictionaries\n",
    "    \"\"\"\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(f\"{directory_path}/*.json\")\n",
    "    print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "    \n",
    "    articles = []\n",
    "    article_hashes = set()  # Store content hashes to detect duplicates\n",
    "    \n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                article = json.load(f)\n",
    "                \n",
    "                # Skip if article doesn't have content\n",
    "                if 'content' not in article or not article['content']:\n",
    "                    print(f\"Skipping {os.path.basename(file)}: No content\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a hash of the article content to detect duplicates\n",
    "                content_hash = hashlib.md5(article['content'].encode('utf-8')).hexdigest()\n",
    "                \n",
    "                if content_hash in article_hashes:\n",
    "                    print(f\"Skipping {os.path.basename(file)}: Duplicate content\")\n",
    "                    continue\n",
    "                \n",
    "                # Add hash to set and article to list\n",
    "                article_hashes.add(content_hash)\n",
    "                articles.append(article)\n",
    "                print(f\"Loaded {os.path.basename(file)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {os.path.basename(file)}: {e}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(articles)} unique articles successfully\")\n",
    "    return articles\n",
    "\n",
    "# Path to raw data directory\n",
    "raw_data_dir = './output/data/raw'\n",
    "\n",
    "# Load unique articles\n",
    "articles = load_articles_from_directory(raw_data_dir)\n",
    "\n",
    "# Extract text content\n",
    "article_texts = [article['content'] for article in articles]\n",
    "article_titles = [article.get('title', 'Untitled') for article in articles]\n",
    "\n",
    "# Show first article\n",
    "if article_texts:\n",
    "    print(f\"Article Title: {article_titles[0] if article_titles[0] else 'No title'}\")\n",
    "    print(f\"\\nRaw Text:\")\n",
    "    print(article_texts[0][:500] + \"...\" if len(article_texts[0]) > 500 else article_texts[0])\n",
    "else:\n",
    "    print(\"No articles with content found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "cleaned_texts = []\n",
    "for text in article_texts:\n",
    "    # Clean the text but keep capitalization for NER\n",
    "    cleaned = clean_text(\n",
    "        text,\n",
    "        lowercase=False,  # Keep case for NER\n",
    "        remove_stops=False,  # Keep stop words for context\n",
    "        lemmatize=False,  # Don't lemmatize to preserve entities\n",
    "    )\n",
    "    cleaned_texts.append(cleaned)\n",
    "\n",
    "# Show first cleaned text\n",
    "print(f\"Cleaned Text:\")\n",
    "print(cleaned_texts[0][:500] + \"...\" if len(cleaned_texts[0]) > 500 else cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER)\n",
    "\n",
    "Let's extract named entities from the preprocessed text using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NER extractors\n",
    "spacy_extractor = SpacyNERExtractor()\n",
    "\n",
    "# Extract entities\n",
    "all_entities = []\n",
    "for i, text in enumerate(cleaned_texts):\n",
    "    entities = spacy_extractor.extract_entities(text)\n",
    "    all_entities.append(entities)\n",
    "    \n",
    "    print(f\"\\nEntities from article {i+1}: {article_titles[i]}\")\n",
    "    for entity, entity_type in entities:\n",
    "        print(f\"- {entity} ({entity_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NER Model Comparison (CRF vs. spaCy)\n",
    "\n",
    "In a full implementation, we would train a CRF model on the CoNLL-2003 dataset and compare it with spaCy's model. For this demonstration, we'll just use the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for model training and evaluation\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import pickle\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Initialize NER extractors\n",
    "spacy_extractor = SpacyNERExtractor()\n",
    "crf_extractor = CRFExtractor('./output/models/crf_ner_model.pkl')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"Training and comparing NER models...\")\n",
    "\n",
    "# 1. Load the CoNLL-2003 dataset\n",
    "print(\"Loading CoNLL-2003 dataset...\")\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "print(f\"Dataset loaded. Train: {len(train_dataset)} examples, Validation: {len(validation_dataset)} examples, Test: {len(test_dataset)} examples\")\n",
    "\n",
    "# Get tag names from train dataset\n",
    "tag_names = train_dataset.features['ner_tags'].feature.names\n",
    "print(f\"NER tag names: {tag_names}\")\n",
    "\n",
    "# 2. Prepare data for CRF model\n",
    "print(\"\\nPreparing data for CRF model...\")\n",
    "max_train_samples = None  # You can limit samples\n",
    "max_val_samples = None\n",
    "max_test_samples = None\n",
    "\n",
    "def word2features(tokens, i):\n",
    "    \"\"\"\n",
    "    Extract features for token at position i in the token list.\n",
    "    This function is specifically designed for the CoNLL dataset token structure.\n",
    "    \"\"\"\n",
    "    token = tokens[i]\n",
    "    \n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word': token,\n",
    "        'word.lower': token.lower(),\n",
    "        'word.isupper': token.isupper(),\n",
    "        'word.istitle': token.istitle(),\n",
    "        'word.isdigit': token.isdigit(),\n",
    "        'position': i,\n",
    "        'length': len(token)\n",
    "    }\n",
    "    \n",
    "    # Add prefix and suffix features\n",
    "    if len(token) > 2:\n",
    "        features['prefix2'] = token[:2]\n",
    "        features['suffix2'] = token[-2:]\n",
    "    if len(token) > 3:\n",
    "        features['prefix3'] = token[:3]\n",
    "        features['suffix3'] = token[-3:]\n",
    "    \n",
    "    # Add features for token position\n",
    "    features['is_first'] = i == 0\n",
    "    features['is_last'] = i == len(tokens) - 1\n",
    "    \n",
    "    # Add features for previous and next tokens\n",
    "    if i > 0:\n",
    "        prev_token = tokens[i-1]\n",
    "        features['prev_word'] = prev_token\n",
    "        features['prev_word.lower'] = prev_token.lower()\n",
    "        features['prev+word.istitle'] = prev_token.istitle()\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    \n",
    "    if i < len(tokens) - 1:\n",
    "        next_token = tokens[i+1]\n",
    "        features['next_word'] = next_token\n",
    "        features['next_word.lower'] = next_token.lower()\n",
    "        features['next_word.istitle'] = next_token.istitle()\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    \n",
    "    return features\n",
    "\n",
    "def sent2features(tokens):\n",
    "    \"\"\"Convert a list of tokens to a list of features.\"\"\"\n",
    "    return [word2features(tokens, i) for i in range(len(tokens))]\n",
    "\n",
    "def sent2labels(sent_labels, tag_names):\n",
    "    \"\"\"Convert numeric labels to BIO tag strings.\"\"\"\n",
    "    return [tag_names[label] for label in sent_labels]\n",
    "\n",
    "def prepare_data_for_crf(dataset_split, tag_names, max_samples=None):\n",
    "    \"\"\"\n",
    "    Prepare features and labels from a dataset split for CRF training.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Use proper indexing to get actual data samples\n",
    "    sample_indices = range(min(len(dataset_split), max_samples or len(dataset_split)))\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        try:\n",
    "            # Get the actual sample data by index\n",
    "            sample = dataset_split[i]\n",
    "            \n",
    "            # Process tokens and labels\n",
    "            tokens = sample['tokens']\n",
    "            ner_tags = sample['ner_tags']\n",
    "            \n",
    "            # Extract features and convert labels\n",
    "            X.append(sent2features(tokens))\n",
    "            y.append(sent2labels(ner_tags, tag_names))\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1}/{len(sample_indices)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Process the dataset\n",
    "X_train, y_train = prepare_data_for_crf(train_dataset, tag_names, max_train_samples)\n",
    "X_val, y_val = prepare_data_for_crf(validation_dataset, tag_names, max_val_samples)\n",
    "X_test, y_test = prepare_data_for_crf(test_dataset, tag_names, max_test_samples)\n",
    "\n",
    "print(f\"Data prepared for CRF: Training set: {len(X_train)} sentences, Validation set: {len(X_val)} sentences, Test set: {len(X_test)} sentences\")\n",
    "\n",
    "# 3. Train CRF model\n",
    "print(\"\\nTraining CRF model...\")\n",
    "try:\n",
    "    # Check if we have training data\n",
    "    if len(X_train) > 0 and len(y_train) > 0:\n",
    "        crf = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "        \n",
    "        crf.fit(X_train, y_train)\n",
    "        print(\"CRF model trained successfully!\")\n",
    "    else:\n",
    "        print(\"No training data available. Creating a simple CRF model with synthetic data.\")\n",
    "        # Create a basic CRF model with minimal synthetic data\n",
    "        crf = CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100\n",
    "        )\n",
    "        \n",
    "        # Create synthetic data that mimics CoNLL structure\n",
    "        X_synthetic = [\n",
    "            [\n",
    "                {'bias': 1.0, 'word': 'John', 'word.istitle': True},\n",
    "                {'bias': 1.0, 'word': 'Smith', 'word.istitle': True},\n",
    "                {'bias': 1.0, 'word': 'works', 'word.islower': True},\n",
    "                {'bias': 1.0, 'word': 'at', 'word.islower': True},\n",
    "                {'bias': 1.0, 'word': 'IBM', 'word.isupper': True},\n",
    "                {'bias': 1.0, 'word': '.', 'word.ispunct': True}\n",
    "            ],\n",
    "            [\n",
    "                {'bias': 1.0, 'word': 'Mary', 'word.istitle': True},\n",
    "                {'bias': 1.0, 'word': 'lives', 'word.islower': True},\n",
    "                {'bias': 1.0, 'word': 'in', 'word.islower': True},\n",
    "                {'bias': 1.0, 'word': 'New', 'word.istitle': True},\n",
    "                {'bias': 1.0, 'word': 'York', 'word.istitle': True},\n",
    "                {'bias': 1.0, 'word': '.', 'word.ispunct': True}\n",
    "            ]\n",
    "        ]\n",
    "        \n",
    "        y_synthetic = [\n",
    "            ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O'],\n",
    "            ['B-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
    "        ]\n",
    "        \n",
    "        crf.fit(X_synthetic, y_synthetic)\n",
    "        print(\"CRF model trained with synthetic data.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error training CRF model: {e}\")\n",
    "    print(\"Creating a simple CRF model with synthetic data.\")\n",
    "    # Create a basic CRF model with minimal synthetic data\n",
    "    crf = CRF(\n",
    "        algorithm='lbfgs',\n",
    "        c1=0.1,\n",
    "        c2=0.1,\n",
    "        max_iterations=100\n",
    "    )\n",
    "    \n",
    "    # Create synthetic data that mimics CoNLL structure\n",
    "    X_synthetic = [\n",
    "        [\n",
    "            {'bias': 1.0, 'word': 'John', 'word.istitle': True},\n",
    "            {'bias': 1.0, 'word': 'Smith', 'word.istitle': True},\n",
    "            {'bias': 1.0, 'word': 'works', 'word.islower': True},\n",
    "            {'bias': 1.0, 'word': 'at', 'word.islower': True},\n",
    "            {'bias': 1.0, 'word': 'IBM', 'word.isupper': True},\n",
    "            {'bias': 1.0, 'word': '.', 'word.ispunct': True}\n",
    "        ],\n",
    "        [\n",
    "            {'bias': 1.0, 'word': 'Mary', 'word.istitle': True},\n",
    "            {'bias': 1.0, 'word': 'lives', 'word.islower': True},\n",
    "            {'bias': 1.0, 'word': 'in', 'word.islower': True},\n",
    "            {'bias': 1.0, 'word': 'New', 'word.istitle': True},\n",
    "            {'bias': 1.0, 'word': 'York', 'word.istitle': True},\n",
    "            {'bias': 1.0, 'word': '.', 'word.ispunct': True}\n",
    "        ]\n",
    "    ]\n",
    "    \n",
    "    y_synthetic = [\n",
    "        ['B-PER', 'I-PER', 'O', 'O', 'B-ORG', 'O'],\n",
    "        ['B-PER', 'O', 'O', 'B-LOC', 'I-LOC', 'O']\n",
    "    ]\n",
    "    \n",
    "    crf.fit(X_synthetic, y_synthetic)\n",
    "    print(\"CRF model trained with synthetic data.\")\n",
    "\n",
    "# 4. Evaluate CRF model\n",
    "print(\"\\nEvaluating CRF model...\")\n",
    "if len(X_test) > 0:\n",
    "    y_pred = crf.predict(X_test)\n",
    "\n",
    "    print(\"\\nCRF Model Evaluation:\")\n",
    "    # Check if we have any predictions or test data\n",
    "    if len(y_test) > 0 and len(y_pred) > 0:\n",
    "        # Get unique labels excluding 'O'\n",
    "        unique_labels = set()\n",
    "        for tags in y_test:\n",
    "            unique_labels.update(tags)\n",
    "        if 'O' in unique_labels:\n",
    "            unique_labels.remove('O')\n",
    "        \n",
    "        # Check if there are any non-O labels\n",
    "        if unique_labels:\n",
    "            try:\n",
    "                crf_report = metrics.flat_classification_report(\n",
    "                    y_test, y_pred, \n",
    "                    labels=list(unique_labels),\n",
    "                    zero_division=0\n",
    "                )\n",
    "                print(crf_report)\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating classification report: {e}\")\n",
    "        else:\n",
    "            print(\"No entity labels found in test data.\")\n",
    "    else:\n",
    "        print(\"No test data or predictions available.\")\n",
    "else:\n",
    "    print(\"No test data available for evaluation.\")\n",
    "\n",
    "# 5. Calculate metrics for CRF model\n",
    "crf_metrics = {}\n",
    "try:\n",
    "    if len(X_test) > 0 and len(y_test) > 0 and len(y_pred) > 0:\n",
    "        # Calculate metrics with zero_division=0 to avoid warnings\n",
    "        crf_metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "        crf_metrics['precision'] = precision_score(y_test, y_pred, zero_division=0)\n",
    "        crf_metrics['recall'] = recall_score(y_test, y_pred, zero_division=0)\n",
    "        crf_metrics['f1'] = f1_score(y_test, y_pred, zero_division=0)\n",
    "        \n",
    "        print(f\"CRF Model Metrics:\")\n",
    "        print(f\"- Accuracy: {crf_metrics['accuracy']:.4f}\")\n",
    "        print(f\"- Precision: {crf_metrics['precision']:.4f}\")\n",
    "        print(f\"- Recall: {crf_metrics['recall']:.4f}\")\n",
    "        print(f\"- F1 Score: {crf_metrics['f1']:.4f}\")\n",
    "    else:\n",
    "        print(\"Cannot calculate CRF metrics: no data available\")\n",
    "        # Use placeholder values for visualization\n",
    "        crf_metrics = {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0\n",
    "        }\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating CRF metrics: {e}\")\n",
    "    # Use placeholder values for visualization\n",
    "    crf_metrics = {\n",
    "        'accuracy': 0.0,\n",
    "        'precision': 0.0,\n",
    "        'recall': 0.0,\n",
    "        'f1': 0.0\n",
    "    }\n",
    "\n",
    "# 6. Evaluate spaCy NER model\n",
    "print(\"\\nEvaluating spaCy NER model...\")\n",
    "\n",
    "def convert_to_bio_tags(text, entities, token_indices):\n",
    "    \"\"\"Convert entities to BIO tags for a tokenized text.\"\"\"\n",
    "    tags = ['O'] * len(token_indices)\n",
    "    \n",
    "    for ent in entities:\n",
    "        ent_start = ent.start_char\n",
    "        ent_end = ent.end_char\n",
    "        ent_type = ent.label_\n",
    "        \n",
    "        # Map spaCy entity types to CoNLL format\n",
    "        if ent_type == 'PERSON':\n",
    "            ent_type = 'PER'\n",
    "        elif ent_type == 'ORG':\n",
    "            ent_type = 'ORG'\n",
    "        elif ent_type in ['GPE', 'LOC']:\n",
    "            ent_type = 'LOC'\n",
    "        else:\n",
    "            ent_type = 'MISC'\n",
    "        \n",
    "        # Find tokens that correspond to this entity\n",
    "        for i, (start, end) in enumerate(token_indices):\n",
    "            # If token is within entity boundaries\n",
    "            if start >= ent_start and start < ent_end:\n",
    "                if start == ent_start:  # First token of entity\n",
    "                    tags[i] = f'B-{ent_type}'\n",
    "                else:  # Continuation of entity\n",
    "                    tags[i] = f'I-{ent_type}'\n",
    "    \n",
    "    return tags\n",
    "\n",
    "def map_spacy_to_conll(spacy_label):\n",
    "    \"\"\"Map spaCy entity types to CoNLL types.\"\"\"\n",
    "    mapping = {\n",
    "        'PERSON': 'PER',\n",
    "        'ORG': 'ORG',\n",
    "        'GPE': 'LOC',  # GPE (countries, cities, etc.) maps to LOC in CoNLL\n",
    "        'LOC': 'LOC',\n",
    "        'PRODUCT': 'MISC',\n",
    "        'EVENT': 'MISC',\n",
    "        'WORK_OF_ART': 'MISC',\n",
    "        'LANGUAGE': 'MISC',\n",
    "        'FAC': 'LOC',  # Facilities often map to LOC\n",
    "        'NORP': 'MISC'  # Nationalities, religious and political groups\n",
    "    }\n",
    "    return mapping.get(spacy_label, None)\n",
    "\n",
    "def evaluate_spacy_ner(test_dataset, tag_names, max_samples=None):\n",
    "    \"\"\"Evaluate spaCy NER on the test dataset.\"\"\"\n",
    "    true_entities_list = []\n",
    "    pred_entities_list = []\n",
    "    \n",
    "    # Use proper indexing to get actual data samples\n",
    "    sample_indices = range(min(len(test_dataset), max_samples or len(test_dataset)))\n",
    "    print(f\"Evaluating spaCy on {len(sample_indices)} samples\")\n",
    "    \n",
    "    for i in sample_indices:\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing sample {i+1}/{len(sample_indices)}\")\n",
    "        \n",
    "        try:\n",
    "            # Get the actual sample data by index\n",
    "            sample = test_dataset[i]\n",
    "            \n",
    "            tokens = sample['tokens']\n",
    "            text = ' '.join(tokens)\n",
    "            ner_tags = sample['ner_tags']\n",
    "            \n",
    "            # Get ground truth entities\n",
    "            true_bio = [tag_names[tag] for tag in ner_tags]\n",
    "            \n",
    "            # Process with spaCy\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            # Convert spaCy entities to BIO tags\n",
    "            pred_bio = ['O'] * len(tokens)\n",
    "            for ent in doc.ents:\n",
    "                # Map spaCy entity types to CoNLL types\n",
    "                ent_type = map_spacy_to_conll(ent.label_)\n",
    "                if not ent_type:\n",
    "                    continue\n",
    "                \n",
    "                # Find token positions that correspond to this entity\n",
    "                start_token = None\n",
    "                end_token = None\n",
    "                curr_pos = 0\n",
    "                \n",
    "                for j, token in enumerate(tokens):\n",
    "                    token_start = curr_pos\n",
    "                    token_end = token_start + len(token)\n",
    "                    \n",
    "                    # Check if this token overlaps with the entity\n",
    "                    if token_start <= ent.start_char < token_end and start_token is None:\n",
    "                        start_token = j\n",
    "                    if token_start < ent.end_char <= token_end:\n",
    "                        end_token = j + 1\n",
    "                        break\n",
    "                    \n",
    "                    curr_pos = token_end + 1  # +1 for the space\n",
    "                \n",
    "                # If we found the entity position, add to BIO tags\n",
    "                if start_token is not None and end_token is not None:\n",
    "                    pred_bio[start_token] = f'B-{ent_type}'\n",
    "                    for j in range(start_token + 1, end_token):\n",
    "                        pred_bio[j] = f'I-{ent_type}'\n",
    "            \n",
    "            true_entities_list.append(true_bio)\n",
    "            pred_entities_list.append(pred_bio)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating sample {i} with spaCy: {e}\")\n",
    "    \n",
    "    return true_entities_list, pred_entities_list\n",
    "\n",
    "# Evaluate spaCy on a small portion of test data\n",
    "true_entities, spacy_predictions = evaluate_spacy_ner(test_dataset, tag_names, max_test_samples)\n",
    "\n",
    "# 7. Calculate metrics for spaCy model\n",
    "spacy_metrics = {}\n",
    "try:\n",
    "    if len(true_entities) > 0 and len(spacy_predictions) > 0:\n",
    "        # Calculate metrics with zero_division=0 to avoid warnings\n",
    "        spacy_metrics['accuracy'] = accuracy_score(true_entities, spacy_predictions)\n",
    "        spacy_metrics['precision'] = precision_score(true_entities, spacy_predictions, zero_division=0)\n",
    "        spacy_metrics['recall'] = recall_score(true_entities, spacy_predictions, zero_division=0)\n",
    "        spacy_metrics['f1'] = f1_score(true_entities, spacy_predictions, zero_division=0)\n",
    "        \n",
    "        print(f\"spaCy NER Model Metrics:\")\n",
    "        print(f\"- Accuracy: {spacy_metrics['accuracy']:.4f}\")\n",
    "        print(f\"- Precision: {spacy_metrics['precision']:.4f}\")\n",
    "        print(f\"- Recall: {spacy_metrics['recall']:.4f}\")\n",
    "        print(f\"- F1 Score: {spacy_metrics['f1']:.4f}\")\n",
    "    else:\n",
    "        print(\"Cannot calculate spaCy metrics: no data available\")\n",
    "        # Use placeholder values for visualization\n",
    "        spacy_metrics = {\n",
    "            'accuracy': 0.0,\n",
    "            'precision': 0.0,\n",
    "            'recall': 0.0,\n",
    "            'f1': 0.0\n",
    "        }\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating spaCy metrics: {e}\")\n",
    "    # Use placeholder values for visualization\n",
    "    spacy_metrics = {\n",
    "        'accuracy': 0.0,\n",
    "        'precision': 0.0,\n",
    "        'recall': 0.0,\n",
    "        'f1': 0.0\n",
    "    }\n",
    "\n",
    "# 8. Compare models\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"{'Metric':<10} {'CRF':<10} {'spaCy':<10}\")\n",
    "print(f\"{'-'*30}\")\n",
    "print(f\"{'Accuracy':<10} {crf_metrics['accuracy']:.4f}     {spacy_metrics['accuracy']:.4f}\")\n",
    "print(f\"{'Precision':<10} {crf_metrics['precision']:.4f}     {spacy_metrics['precision']:.4f}\")\n",
    "print(f\"{'Recall':<10} {crf_metrics['recall']:.4f}     {spacy_metrics['recall']:.4f}\")\n",
    "print(f\"{'F1 Score':<10} {crf_metrics['f1']:.4f}     {spacy_metrics['f1']:.4f}\")\n",
    "\n",
    "# 9. Visualize comparison\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "crf_scores = [crf_metrics['accuracy'], crf_metrics['precision'], crf_metrics['recall'], crf_metrics['f1']]\n",
    "spacy_scores = [spacy_metrics['accuracy'], spacy_metrics['precision'], spacy_metrics['recall'], spacy_metrics['f1']]\n",
    "\n",
    "# Set up plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(metrics_names))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(index, crf_scores, bar_width, label='CRF')\n",
    "plt.bar(index + bar_width, spacy_scores, bar_width, label='spaCy')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('NER Models Comparison (CRF vs spaCy)')\n",
    "plt.xticks(index + bar_width / 2, metrics_names)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add values on top of bars\n",
    "for i, v in enumerate(crf_scores):\n",
    "    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "for i, v in enumerate(spacy_scores):\n",
    "    plt.text(i + bar_width, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/visualization/ner_model_comparison.png')\n",
    "plt.show()\n",
    "\n",
    "# 10. Save the trained CRF model\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "\n",
    "# Save CRF model\n",
    "with open('output/models/crf_ner_model.pkl', 'wb') as f:\n",
    "    pickle.dump(crf, f)\n",
    "\n",
    "print(\"\\nCRF model saved to output/models/crf_ner_model.pkl\")\n",
    "\n",
    "# Now we'll update the CRF extractor with our trained model\n",
    "crf_extractor.model = crf\n",
    "print(\"CRF model loaded into extractor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Extract Entities from Articles\n",
    "\n",
    "Now that we have both models ready, let's extract named entities from our articles using both the spaCy model and our trained CRF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare features for the CRF model\n",
    "def extract_features_from_text(text):\n",
    "    \"\"\"Extract features for CRF from text.\"\"\"\n",
    "    # Split text into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Generate features for each token\n",
    "    features = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Basic features\n",
    "        feature = {\n",
    "            'bias': 1.0,\n",
    "            'word': token,\n",
    "            'word.lower': token.lower(),\n",
    "            'word.isupper': token.isupper(),\n",
    "            'word.istitle': token.istitle(),\n",
    "            'is_first': i == 0,\n",
    "            'is_last': i == len(tokens) - 1,\n",
    "            'length': len(token)\n",
    "        }\n",
    "        \n",
    "        # Add prefix/suffix features\n",
    "        if len(token) > 2:\n",
    "            feature['suffix2'] = token[-2:]\n",
    "            feature['prefix2'] = token[:2]\n",
    "        if len(token) > 3:\n",
    "            feature['suffix3'] = token[-3:]\n",
    "            feature['prefix3'] = token[:3]\n",
    "        \n",
    "        # Previous token feature (if available)\n",
    "        if i > 0:\n",
    "            feature['prev_word'] = tokens[i-1]\n",
    "            feature['prev_word.lower'] = tokens[i-1].lower()\n",
    "        else:\n",
    "            feature['BOS'] = True\n",
    "        \n",
    "        # Next token feature (if available)\n",
    "        if i < len(tokens) - 1:\n",
    "            feature['next_word'] = tokens[i+1]\n",
    "            feature['next_word.lower'] = tokens[i+1].lower()\n",
    "        else:\n",
    "            feature['EOS'] = True\n",
    "        \n",
    "        features.append(feature)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to convert CRF predictions to entity tuples\n",
    "def convert_crf_predictions_to_entities(text, predictions):\n",
    "    \"\"\"Convert CRF predictions to (entity_text, entity_type) tuples.\"\"\"\n",
    "    tokens = text.split()\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, (token, tag) in enumerate(zip(tokens, predictions)):\n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append((current_entity['text'], current_entity['type']))\n",
    "            current_entity = {'text': token, 'type': tag[2:]}\n",
    "        elif tag.startswith('I-') and current_entity and current_entity['type'] == tag[2:]:\n",
    "            current_entity['text'] += ' ' + token\n",
    "        elif tag == 'O':\n",
    "            if current_entity:\n",
    "                entities.append((current_entity['text'], current_entity['type']))\n",
    "                current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append((current_entity['text'], current_entity['type']))\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities using both models\n",
    "all_spacy_entities = []\n",
    "all_crf_entities = []\n",
    "\n",
    "print(\"\\nExtracting entities from articles:\")\n",
    "for i, text in enumerate(cleaned_texts):\n",
    "    # SpaCy extraction - this is reliable since we're using spaCy's built-in NER\n",
    "    spacy_entities = spacy_extractor.extract_entities(text)\n",
    "    all_spacy_entities.append(spacy_entities)\n",
    "    \n",
    "    # CRF extraction - use our trained model or fallback to spaCy with mapping\n",
    "    try:\n",
    "        # Try to use the CRF model if it's properly trained\n",
    "        if hasattr(crf_extractor, 'model') and crf_extractor.model is not None:\n",
    "            # Prepare features\n",
    "            features = extract_features_from_text(text)\n",
    "            \n",
    "            # Make predictions - wrap in try/except in case it fails\n",
    "            try:\n",
    "                predictions = crf_extractor.model.predict([features])[0]\n",
    "                crf_entities = convert_crf_predictions_to_entities(text, predictions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting with CRF model: {e}\")\n",
    "                # Fallback to spaCy with CoNLL mapping\n",
    "                doc = nlp(text)\n",
    "                crf_entities = []\n",
    "                for ent in doc.ents:\n",
    "                    if ent.label_ == \"PERSON\":\n",
    "                        mapped_type = \"PER\"\n",
    "                    elif ent.label_ == \"ORG\":\n",
    "                        mapped_type = \"ORG\"\n",
    "                    elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "                        mapped_type = \"LOC\"\n",
    "                    else:\n",
    "                        mapped_type = \"MISC\"\n",
    "                    crf_entities.append((ent.text, mapped_type))\n",
    "        else:\n",
    "            # No CRF model, use spaCy with CoNLL mapping\n",
    "            doc = nlp(text)\n",
    "            crf_entities = []\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"PERSON\":\n",
    "                    mapped_type = \"PER\"\n",
    "                elif ent.label_ == \"ORG\":\n",
    "                    mapped_type = \"ORG\"\n",
    "                elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "                    mapped_type = \"LOC\"\n",
    "                else:\n",
    "                    mapped_type = \"MISC\"\n",
    "                crf_entities.append((ent.text, mapped_type))\n",
    "                \n",
    "        all_crf_entities.append(crf_entities)\n",
    "        \n",
    "        # Display results for a few articles\n",
    "        if i < 3:\n",
    "            print(f\"\\nEntities from article {i+1}: {article_titles[i]}\")\n",
    "            print(\"SpaCy entities:\")\n",
    "            for entity, entity_type in spacy_entities[:10]:  # Show first 10 for brevity\n",
    "                print(f\"- {entity} ({entity_type})\")\n",
    "            if len(spacy_entities) > 10:\n",
    "                print(f\"  ... and {len(spacy_entities) - 10} more\")\n",
    "            \n",
    "            print(\"\\nCRF entities:\")\n",
    "            for entity, entity_type in crf_entities[:10]:  # Show first 10 for brevity\n",
    "                print(f\"- {entity} ({entity_type})\")\n",
    "            if len(crf_entities) > 10:\n",
    "                print(f\"  ... and {len(crf_entities) - 10} more\")\n",
    "        \n",
    "        if i == 3:  # After showing detailed output for 3 articles\n",
    "            print(\"\\nProcessing remaining articles...\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting entities from article {i+1}: {e}\")\n",
    "        all_crf_entities.append([])\n",
    "\n",
    "# Compare entity counts\n",
    "spacy_entity_count = sum(len(entities) for entities in all_spacy_entities)\n",
    "crf_entity_count = sum(len(entities) for entities in all_crf_entities)\n",
    "\n",
    "print(f\"\\nTotal entities found:\")\n",
    "print(f\"- SpaCy: {spacy_entity_count} entities\")\n",
    "print(f\"- CRF: {crf_entity_count} entities\")\n",
    "\n",
    "# Visualize entity type distribution\n",
    "def plot_entity_distribution(all_entities, model_name):\n",
    "    entity_types = {}\n",
    "    for article_entities in all_entities:\n",
    "        for entity, entity_type in article_entities:\n",
    "            if entity_type not in entity_types:\n",
    "                entity_types[entity_type] = 0\n",
    "            entity_types[entity_type] += 1\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(entity_types.keys(), entity_types.values())\n",
    "    plt.title(f'Entity Type Distribution - {model_name}')\n",
    "    plt.xlabel('Entity Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/visualization/entity_distribution_{model_name.lower()}.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_entity_distribution(all_spacy_entities, \"SpaCy\")\n",
    "plot_entity_distribution(all_crf_entities, \"CRF\")\n",
    "\n",
    "# For the rest of the pipeline, we'll use a combined approach:\n",
    "# - For entity types that spaCy handles well (PERSON, ORG, GPE), we'll use spaCy\n",
    "# - For entity types that CRF handles better (like specific CoNLL categories), we'll use CRF\n",
    "# - We'll merge the results to get the best of both models\n",
    "\n",
    "# Merge entities from both models (removing duplicates)\n",
    "all_merged_entities = []\n",
    "\n",
    "for i, (spacy_entities, crf_entities) in enumerate(zip(all_spacy_entities, all_crf_entities)):\n",
    "    # Create a combined set of entities\n",
    "    seen_entities = set()\n",
    "    merged_entities = []\n",
    "    \n",
    "    # First add spaCy entities (they tend to have better precision)\n",
    "    for entity, entity_type in spacy_entities:\n",
    "        entity_key = (entity.lower(), entity_type)\n",
    "        if entity_key not in seen_entities:\n",
    "            seen_entities.add(entity_key)\n",
    "            merged_entities.append((entity, entity_type))\n",
    "    \n",
    "    # Then add CRF entities if they don't overlap with spaCy\n",
    "    for entity, entity_type in crf_entities:\n",
    "        # Map CoNLL entity types to spaCy types for consistency\n",
    "        mapped_type = entity_type\n",
    "        if entity_type == 'PER':\n",
    "            mapped_type = 'PERSON'\n",
    "        elif entity_type == 'LOC':\n",
    "            mapped_type = 'GPE'  # Simplification, could be LOC or GPE\n",
    "        \n",
    "        entity_key = (entity.lower(), mapped_type)\n",
    "        if entity_key not in seen_entities:\n",
    "            seen_entities.add(entity_key)\n",
    "            merged_entities.append((entity, mapped_type))\n",
    "    \n",
    "    all_merged_entities.append(merged_entities)\n",
    "\n",
    "# Use the merged entities for the rest of the pipeline\n",
    "all_entities = all_merged_entities\n",
    "\n",
    "merged_entity_count = sum(len(entities) for entities in all_merged_entities)\n",
    "print(f\"\\nTotal merged entities: {merged_entity_count} entities\")\n",
    "\n",
    "# Plot merged entity distribution\n",
    "plot_entity_distribution(all_merged_entities, \"Merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relation Extraction\n",
    "\n",
    "Now let's extract relations between the entities we identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize relation extractor\n",
    "relation_extractor = SpacyRelationExtractor()\n",
    "\n",
    "# Extract relations\n",
    "all_relations = []\n",
    "for i, text in enumerate(cleaned_texts):\n",
    "    relations = relation_extractor.extract_relations(text)\n",
    "    \n",
    "    # Filter out self-relations\n",
    "    filtered_relations = []\n",
    "    for subject, predicate, obj in relations:\n",
    "        # Skip if subject and object are the same entity\n",
    "        if subject[0] != obj[0]:\n",
    "            filtered_relations.append((subject, predicate, obj))\n",
    "    \n",
    "    all_relations.append(filtered_relations)\n",
    "    \n",
    "    print(f\"\\nRelations from article {i+1}: {article_titles[i]}\")\n",
    "    for subject, predicate, obj in filtered_relations:\n",
    "        print(f\"- {subject[0]} ({subject[1]}) --[{predicate}]--> {obj[0]} ({obj[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Knowledge Graph Construction\n",
    "\n",
    "Now let's build a knowledge graph from the extracted entities and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge graph builder\n",
    "kg_builder = KnowledgeGraphBuilder(namespace=\"http://example.org/graphify/\")\n",
    "\n",
    "# Collect all entities and relations\n",
    "all_kg_entities = set()\n",
    "all_kg_relations = []\n",
    "\n",
    "for entities in all_entities:\n",
    "    for entity in entities:\n",
    "        all_kg_entities.add(entity)\n",
    "\n",
    "for relations in all_relations:\n",
    "    all_kg_relations.extend(relations)\n",
    "\n",
    "# Add to knowledge graph\n",
    "kg_builder.add_entities_and_relations(list(all_kg_entities), all_kg_relations)\n",
    "\n",
    "# Save knowledge graph to file\n",
    "kg_file = \"output/data/knowledge_graph.ttl\"\n",
    "kg_builder.save_to_file(kg_file, format=\"turtle\")\n",
    "print(f\"\\nKnowledge graph saved to {kg_file}\")\n",
    "\n",
    "# Display stats\n",
    "print(f\"\\nKnowledge Graph Statistics:\")\n",
    "print(f\"- Entities: {len(all_kg_entities)}\")\n",
    "print(f\"- Relations: {len(all_kg_relations)}\")\n",
    "print(f\"- Triples: {len(kg_builder.get_triples())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Let's create visualizations of our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "# Before visualizing, filter out nodes without relations\n",
    "\n",
    "# Get all entities that participate in relationships\n",
    "connected_entities = set()\n",
    "for relations in all_relations:\n",
    "    for subject, predicate, obj in relations:\n",
    "        connected_entities.add(subject[0])  # Add subject text\n",
    "        connected_entities.add(obj[0])      # Add object text\n",
    "\n",
    "# Filter the entities to only include those with relations\n",
    "connected_kg_entities = [entity for entity in all_kg_entities if entity[0] in connected_entities]\n",
    "\n",
    "# Now use the filtered entities for visualization\n",
    "kg_builder = KnowledgeGraphBuilder(namespace=\"http://example.org/graphify/\")\n",
    "kg_builder.add_entities_and_relations(connected_kg_entities, all_kg_relations)\n",
    "\n",
    "# Static plot\n",
    "plot_file = \"output/visualization/knowledge_graph_plot.png\"\n",
    "kg_builder.plot_graph(plot_file)\n",
    "print(f\"Static plot saved to {plot_file}\")\n",
    "\n",
    "# Interactive visualization\n",
    "vis_file = \"output/visualization/knowledge_graph_interactive.html\"\n",
    "kg_builder.visualize(vis_file)\n",
    "print(f\"Interactive visualization saved to {vis_file}\")\n",
    "\n",
    "# Display the static plot in the notebook\n",
    "from IPython.display import Image\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Knowledge Graph\n",
    "\n",
    "Let's run some SPARQL queries on our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query: Find all organizations and their locations\n",
    "query1 = \"\"\"\n",
    "PREFIX ns: <http://example.org/graphify/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT ?org ?loc\n",
    "WHERE {\n",
    "    ?org_uri rdf:type ns:ORG .\n",
    "    ?loc_uri rdf:type ns:GPE .\n",
    "    ?org_uri ?pred ?loc_uri .\n",
    "    ?org_uri <http://www.w3.org/2000/01/rdf-schema#label> ?org .\n",
    "    ?loc_uri <http://www.w3.org/2000/01/rdf-schema#label> ?loc .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results1 = kg_builder.query_sparql(query1)\n",
    "print(\"Organizations and their locations:\")\n",
    "for row in results1:\n",
    "    print(f\"- {row[0]} is located in {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exporting Results for Further Analysis\n",
    "\n",
    "Let's export the entities and relations to CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export entities to CSV\n",
    "entity_data = []\n",
    "for entity_text, entity_type in all_kg_entities:\n",
    "    entity_data.append({'entity': entity_text, 'type': entity_type})\n",
    "\n",
    "entity_df = pd.DataFrame(entity_data)\n",
    "entity_csv = \"output/data/entities.csv\"\n",
    "entity_df.to_csv(entity_csv, index=False)\n",
    "print(f\"Entities exported to {entity_csv}\")\n",
    "\n",
    "# Export relations to CSV\n",
    "relation_data = []\n",
    "for subject, predicate, obj in all_kg_relations:\n",
    "    relation_data.append({\n",
    "        'subject': subject[0],\n",
    "        'subject_type': subject[1],\n",
    "        'predicate': predicate,\n",
    "        'object': obj[0],\n",
    "        'object_type': obj[1]\n",
    "    })\n",
    "\n",
    "relation_df = pd.DataFrame(relation_data)\n",
    "relation_csv = \"output/data/relations.csv\"\n",
    "relation_df.to_csv(relation_csv, index=False)\n",
    "print(f\"Relations exported to {relation_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for building a knowledge graph from raw text data:\n",
    "\n",
    "1. We collected articles using a web scraper\n",
    "2. We preprocessed and cleaned the text\n",
    "3. We extracted named entities using spaCy\n",
    "4. We extracted relations between entities\n",
    "5. We built a knowledge graph from the entities and relations\n",
    "6. We visualized and queried the knowledge graph\n",
    "\n",
    "This pipeline can be extended by:\n",
    "- Training a CRF model for NER and comparing it with spaCy's performance\n",
    "- Implementing more sophisticated relation extraction methods\n",
    "- Adding more data sources\n",
    "- Expanding the knowledge graph with additional entity types and relations\n",
    "- Developing applications that use the knowledge graph for search, recommendation, or other tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
