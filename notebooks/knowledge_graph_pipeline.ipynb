{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Construction Pipeline\n",
    "\n",
    "This notebook demonstrates the complete process of building a knowledge graph from raw text data. The pipeline includes:\n",
    "\n",
    "1. Text collection through web scraping\n",
    "2. Text preprocessing and cleaning\n",
    "3. Named Entity Recognition (NER)\n",
    "4. Relation Extraction (RE)\n",
    "5. Knowledge Graph Construction\n",
    "6. Visualization and querying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for importing local modules\n",
    "# Adjust this path if needed\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.data_collection.scraper import NewsArticleScraper\n",
    "from src.preprocessing.cleaner import clean_text\n",
    "from src.entity_recognition.ner import SpacyNERExtractor, CRFExtractor\n",
    "from src.entity_recognition.comparison import NERComparison\n",
    "from src.relation_extraction.extractor import SpacyRelationExtractor\n",
    "from src.knowledge_graph.builder import KnowledgeGraphBuilder\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('output/data', exist_ok=True)\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "os.makedirs('output/visualization', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "\n",
    "Let's collect news articles from Reuters using our web scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper\n",
    "scraper = NewsArticleScraper(output_dir='output/data/raw')\n",
    "\n",
    "# Scrape articles\n",
    "# Note: This might take a while and might be rate-limited by the website\n",
    "# For demonstration, we can scrape a smaller number of articles\n",
    "# We can also use example data if scraping fails\n",
    "try:\n",
    "    article_files = scraper.scrape_reuters(num_articles=15, category='business')\n",
    "    print(f\"Scraped {len(article_files)} articles:\")\n",
    "    for file in article_files:\n",
    "        print(f\"- {os.path.basename(file)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error scraping articles: {e}\")\n",
    "    # Use example data if scraping fails\n",
    "    logger.info(\"Using example data instead\")\n",
    "    # Create a simple example article\n",
    "    example_article = {\n",
    "        \"id\": \"example1\",\n",
    "        \"title\": \"Apple announces new partnership with Microsoft\",\n",
    "        \"url\": \"https://example.com/article1\",\n",
    "        \"source\": \"example\",\n",
    "        \"category\": \"business\",\n",
    "        \"published_date\": \"2023-01-01\",\n",
    "        \"scraped_date\": \"2023-01-02\",\n",
    "        \"content\": \"Apple Inc. has announced a new partnership with Microsoft Corporation, according to CEO Tim Cook. \\\n",
    "The collaboration will focus on cloud computing services and AI integration. \\\n",
    "The partnership was revealed at a press conference in Cupertino, California yesterday. \\\n",
    "Microsoft CEO Satya Nadella expressed excitement about working with the iPhone maker. \\\n",
    "Apple was founded by Steve Jobs in 1976 and has become one of the world's most valuable companies.\"\n",
    "    }\n",
    "    \n",
    "    # Save example article\n",
    "    os.makedirs('output/data/raw', exist_ok=True)\n",
    "    example_file = 'output/data/raw/example_article.json'\n",
    "    with open(example_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(example_article, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    article_files = [example_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Now let's preprocess the text from the articles we collected.\n",
    "We retrieve the .json files and check for duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles from raw data directory with duplicate detection\n",
    "import glob\n",
    "import hashlib\n",
    "\n",
    "def load_articles_from_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Load JSON articles from a directory with duplicate detection.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing JSON files\n",
    "        \n",
    "    Returns:\n",
    "        list: List of unique article dictionaries\n",
    "    \"\"\"\n",
    "    # Get all JSON files in the directory\n",
    "    json_files = glob.glob(f\"{directory_path}/*.json\")\n",
    "    print(f\"Found {len(json_files)} JSON files in {directory_path}\")\n",
    "    \n",
    "    articles = []\n",
    "    article_hashes = set()  # Store content hashes to detect duplicates\n",
    "    \n",
    "    for file in json_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                article = json.load(f)\n",
    "                \n",
    "                # Skip if article doesn't have content\n",
    "                if 'content' not in article or not article['content']:\n",
    "                    print(f\"Skipping {os.path.basename(file)}: No content\")\n",
    "                    continue\n",
    "                \n",
    "                # Create a hash of the article content to detect duplicates\n",
    "                content_hash = hashlib.md5(article['content'].encode('utf-8')).hexdigest()\n",
    "                \n",
    "                if content_hash in article_hashes:\n",
    "                    print(f\"Skipping {os.path.basename(file)}: Duplicate content\")\n",
    "                    continue\n",
    "                \n",
    "                # Add hash to set and article to list\n",
    "                article_hashes.add(content_hash)\n",
    "                articles.append(article)\n",
    "                print(f\"Loaded {os.path.basename(file)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {os.path.basename(file)}: {e}\")\n",
    "    \n",
    "    print(f\"\\nLoaded {len(articles)} unique articles successfully\")\n",
    "    return articles\n",
    "\n",
    "# Path to raw data directory\n",
    "raw_data_dir = './output/data/raw'\n",
    "\n",
    "# Load unique articles\n",
    "articles = load_articles_from_directory(raw_data_dir)\n",
    "\n",
    "# Extract text content\n",
    "article_texts = [article['content'] for article in articles]\n",
    "article_titles = [article.get('title', 'Untitled') for article in articles]\n",
    "\n",
    "# Show first article\n",
    "if article_texts:\n",
    "    print(f\"Article Title: {article_titles[0] if article_titles[0] else 'No title'}\")\n",
    "    print(f\"\\nRaw Text:\")\n",
    "    print(article_texts[0][:500] + \"...\" if len(article_texts[0]) > 500 else article_texts[0])\n",
    "else:\n",
    "    print(\"No articles with content found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "cleaned_texts = []\n",
    "for text in article_texts:\n",
    "    # Clean the text but keep capitalization for NER\n",
    "    cleaned = clean_text(\n",
    "        text,\n",
    "        lowercase=False,  # Keep case for NER\n",
    "        remove_stops=False,  # Keep stop words for context\n",
    "        lemmatize=False,  # Don't lemmatize to preserve entities\n",
    "    )\n",
    "    cleaned_texts.append(cleaned)\n",
    "\n",
    "# Show first cleaned text\n",
    "print(f\"Cleaned Text:\")\n",
    "print(cleaned_texts[0][:500] + \"...\" if len(cleaned_texts[0]) > 500 else cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Named Entity Recognition (NER)\n",
    "\n",
    "Let's extract named entities from the preprocessed text using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NER extractors\n",
    "spacy_extractor = SpacyNERExtractor()\n",
    "\n",
    "# Extract entities\n",
    "all_entities = []\n",
    "for i, text in enumerate(cleaned_texts):\n",
    "    entities = spacy_extractor.extract_entities(text)\n",
    "    all_entities.append(entities)\n",
    "    \n",
    "    print(f\"\\nEntities from article {i+1}: {article_titles[i]}\")\n",
    "    for entity, entity_type in entities:\n",
    "        print(f\"- {entity} ({entity_type})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 NER Model Comparison (CRF vs. spaCy)\n",
    "\n",
    "In a full implementation, we would train a CRF model on the CoNLL-2003 dataset and compare it with spaCy's model. For this demonstration, we'll just use the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, let's simulate a CRF model comparison\n",
    "print(\"Note: In a complete implementation, we would train a CRF model \")\n",
    "print(\"on the CoNLL-2003 dataset and compare it with spaCy's model.\")\n",
    "print(\"For simplicity in this demonstration, we're only using spaCy's model.\")\n",
    "\n",
    "# Display entity distribution\n",
    "entity_types = {}\n",
    "for article_entities in all_entities:\n",
    "    for entity, entity_type in article_entities:\n",
    "        if entity_type not in entity_types:\n",
    "            entity_types[entity_type] = 0\n",
    "        entity_types[entity_type] += 1\n",
    "\n",
    "# Plot entity type distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(entity_types.keys(), entity_types.values())\n",
    "plt.title('Entity Type Distribution')\n",
    "plt.xlabel('Entity Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relation Extraction\n",
    "\n",
    "Now let's extract relations between the entities we identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize relation extractor\n",
    "relation_extractor = SpacyRelationExtractor()\n",
    "\n",
    "# Extract relations\n",
    "all_relations = []\n",
    "for i, text in enumerate(cleaned_texts):\n",
    "    relations = relation_extractor.extract_relations(text)\n",
    "    \n",
    "    # Filter out self-relations\n",
    "    filtered_relations = []\n",
    "    for subject, predicate, obj in relations:\n",
    "        # Skip if subject and object are the same entity\n",
    "        if subject[0] != obj[0]:\n",
    "            filtered_relations.append((subject, predicate, obj))\n",
    "    \n",
    "    all_relations.append(filtered_relations)\n",
    "    \n",
    "    print(f\"\\nRelations from article {i+1}: {article_titles[i]}\")\n",
    "    for subject, predicate, obj in filtered_relations:\n",
    "        print(f\"- {subject[0]} ({subject[1]}) --[{predicate}]--> {obj[0]} ({obj[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Knowledge Graph Construction\n",
    "\n",
    "Now let's build a knowledge graph from the extracted entities and relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize knowledge graph builder\n",
    "kg_builder = KnowledgeGraphBuilder(namespace=\"http://example.org/graphify/\")\n",
    "\n",
    "# Collect all entities and relations\n",
    "all_kg_entities = set()\n",
    "all_kg_relations = []\n",
    "\n",
    "for entities in all_entities:\n",
    "    for entity in entities:\n",
    "        all_kg_entities.add(entity)\n",
    "\n",
    "for relations in all_relations:\n",
    "    all_kg_relations.extend(relations)\n",
    "\n",
    "# Add to knowledge graph\n",
    "kg_builder.add_entities_and_relations(list(all_kg_entities), all_kg_relations)\n",
    "\n",
    "# Save knowledge graph to file\n",
    "kg_file = \"output/data/knowledge_graph.ttl\"\n",
    "kg_builder.save_to_file(kg_file, format=\"turtle\")\n",
    "print(f\"\\nKnowledge graph saved to {kg_file}\")\n",
    "\n",
    "# Display stats\n",
    "print(f\"\\nKnowledge Graph Statistics:\")\n",
    "print(f\"- Entities: {len(all_kg_entities)}\")\n",
    "print(f\"- Relations: {len(all_kg_relations)}\")\n",
    "print(f\"- Triples: {len(kg_builder.get_triples())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Let's create visualizations of our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "# Before visualizing, filter out nodes without relations\n",
    "\n",
    "# Get all entities that participate in relationships\n",
    "connected_entities = set()\n",
    "for relations in all_relations:\n",
    "    for subject, predicate, obj in relations:\n",
    "        connected_entities.add(subject[0])  # Add subject text\n",
    "        connected_entities.add(obj[0])      # Add object text\n",
    "\n",
    "# Filter the entities to only include those with relations\n",
    "connected_kg_entities = [entity for entity in all_kg_entities if entity[0] in connected_entities]\n",
    "\n",
    "# Now use the filtered entities for visualization\n",
    "kg_builder = KnowledgeGraphBuilder(namespace=\"http://example.org/graphify/\")\n",
    "kg_builder.add_entities_and_relations(connected_kg_entities, all_kg_relations)\n",
    "\n",
    "# Static plot\n",
    "plot_file = \"output/visualization/knowledge_graph_plot.png\"\n",
    "kg_builder.plot_graph(plot_file)\n",
    "print(f\"Static plot saved to {plot_file}\")\n",
    "\n",
    "# Interactive visualization\n",
    "vis_file = \"output/visualization/knowledge_graph_interactive.html\"\n",
    "kg_builder.visualize(vis_file)\n",
    "print(f\"Interactive visualization saved to {vis_file}\")\n",
    "\n",
    "# Display the static plot in the notebook\n",
    "from IPython.display import Image\n",
    "Image(filename=plot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Querying the Knowledge Graph\n",
    "\n",
    "Let's run some SPARQL queries on our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query 1: Find all organizations and their locations\n",
    "query1 = \"\"\"\n",
    "PREFIX ns: <http://example.org/graphify/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT ?org ?loc\n",
    "WHERE {\n",
    "    ?org_uri rdf:type ns:ORG .\n",
    "    ?loc_uri rdf:type ns:GPE .\n",
    "    ?org_uri ?pred ?loc_uri .\n",
    "    ?org_uri <http://www.w3.org/2000/01/rdf-schema#label> ?org .\n",
    "    ?loc_uri <http://www.w3.org/2000/01/rdf-schema#label> ?loc .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results1 = kg_builder.query_sparql(query1)\n",
    "print(\"Organizations and their locations:\")\n",
    "for row in results1:\n",
    "    print(f\"- {row[0]} is located in {row[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query 2: Find all people and their organizations\n",
    "query2 = \"\"\"\n",
    "PREFIX ns: <http://example.org/graphify/>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "\n",
    "SELECT ?person ?org\n",
    "WHERE {\n",
    "    ?person_uri rdf:type ns:PERSON .\n",
    "    ?org_uri rdf:type ns:ORG .\n",
    "    ?pred_uri ?person_uri ?org_uri .\n",
    "    ?person_uri <http://www.w3.org/2000/01/rdf-schema#label> ?person .\n",
    "    ?org_uri <http://www.w3.org/2000/01/rdf-schema#label> ?org .\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results2 = kg_builder.query_sparql(query2)\n",
    "print(\"People and their organizations:\")\n",
    "for row in results2:\n",
    "    print(f\"- {row[0]} is associated with {row[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exporting Results for Further Analysis\n",
    "\n",
    "Let's export the entities and relations to CSV files for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export entities to CSV\n",
    "entity_data = []\n",
    "for entity_text, entity_type in all_kg_entities:\n",
    "    entity_data.append({'entity': entity_text, 'type': entity_type})\n",
    "\n",
    "entity_df = pd.DataFrame(entity_data)\n",
    "entity_csv = \"output/data/entities.csv\"\n",
    "entity_df.to_csv(entity_csv, index=False)\n",
    "print(f\"Entities exported to {entity_csv}\")\n",
    "\n",
    "# Export relations to CSV\n",
    "relation_data = []\n",
    "for subject, predicate, obj in all_kg_relations:\n",
    "    relation_data.append({\n",
    "        'subject': subject[0],\n",
    "        'subject_type': subject[1],\n",
    "        'predicate': predicate,\n",
    "        'object': obj[0],\n",
    "        'object_type': obj[1]\n",
    "    })\n",
    "\n",
    "relation_df = pd.DataFrame(relation_data)\n",
    "relation_csv = \"output/data/relations.csv\"\n",
    "relation_df.to_csv(relation_csv, index=False)\n",
    "print(f\"Relations exported to {relation_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete pipeline for building a knowledge graph from raw text data:\n",
    "\n",
    "1. We collected articles using a web scraper\n",
    "2. We preprocessed and cleaned the text\n",
    "3. We extracted named entities using spaCy\n",
    "4. We extracted relations between entities\n",
    "5. We built a knowledge graph from the entities and relations\n",
    "6. We visualized and queried the knowledge graph\n",
    "\n",
    "This pipeline can be extended by:\n",
    "- Training a CRF model for NER and comparing it with spaCy's performance\n",
    "- Implementing more sophisticated relation extraction methods\n",
    "- Adding more data sources\n",
    "- Expanding the knowledge graph with additional entity types and relations\n",
    "- Developing applications that use the knowledge graph for search, recommendation, or other tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
