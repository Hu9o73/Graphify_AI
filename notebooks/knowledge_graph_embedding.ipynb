{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Embedding\n",
    "\n",
    "This notebook demonstrates how to create and evaluate knowledge graph embeddings from the knowledge graph constructed in the previous step. It includes:\n",
    "\n",
    "1. Data augmentation to address limited entity count\n",
    "2. Knowledge graph embedding training\n",
    "3. Evaluation and visualization of embeddings\n",
    "4. Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the environment and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for importing local modules\n",
    "# Adjust this path if needed\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.knowledge_graph.augmentation import KnowledgeGraphAugmenter, augment_knowledge_graph\n",
    "from src.knowledge_graph.embeddings import KnowledgeGraphEmbedder, create_and_train_embeddings\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('output/data', exist_ok=True)\n",
    "os.makedirs('output/models', exist_ok=True)\n",
    "os.makedirs('output/embeddings', exist_ok=True)\n",
    "os.makedirs('output/visualization', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine the Original Knowledge Graph\n",
    "\n",
    "First, let's load the knowledge graph we constructed in the previous step and examine its properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Path to the original knowledge graph\n",
    "# Adjust this path to match your file\n",
    "original_kg_path = './output/data/knowledge_graph.ttl'\n",
    "\n",
    "# Load the graph\n",
    "original_graph = Graph()\n",
    "original_graph.parse(original_kg_path, format='turtle')\n",
    "\n",
    "print(f\"Loaded knowledge graph with {len(original_graph)} triples\")\n",
    "\n",
    "# Count entities by type\n",
    "from rdflib.namespace import RDF\n",
    "\n",
    "entity_types = {}\n",
    "for s, p, o in original_graph.triples((None, RDF.type, None)):\n",
    "    entity_type = str(o).split('/')[-1]\n",
    "    if entity_type not in entity_types:\n",
    "        entity_types[entity_type] = 0\n",
    "    entity_types[entity_type] += 1\n",
    "\n",
    "print(\"\\nEntity types and counts:\")\n",
    "for entity_type, count in entity_types.items():\n",
    "    print(f\"- {entity_type}: {count}\")\n",
    "\n",
    "# Count relations\n",
    "relations = {}\n",
    "for s, p, o in original_graph:\n",
    "    if p != RDF.type:\n",
    "        relation = str(p).split('/')[-1]\n",
    "        if relation not in relations:\n",
    "            relations[relation] = 0\n",
    "        relations[relation] += 1\n",
    "\n",
    "print(f\"\\nNumber of distinct relations: {len(relations)}\")\n",
    "print(\"Top 10 most common relations:\")\n",
    "for relation, count in sorted(relations.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"- {relation}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation\n",
    "\n",
    "To address the challenge of limited entity count, we'll augment our knowledge graph with external knowledge from DBpedia and Wikidata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the augmented knowledge graph\n",
    "augmented_kg_path = './output/data/knowledge_graph_augmented.ttl'\n",
    "\n",
    "# Create an augmenter\n",
    "augmenter = KnowledgeGraphAugmenter(namespace=\"http://example.org/graphify/\")\n",
    "\n",
    "# Load the original knowledge graph\n",
    "augmenter.load_graph(original_kg_path)\n",
    "\n",
    "# Extract entities\n",
    "entities = augmenter.extract_entities()\n",
    "print(f\"Extracted {len(entities)} entities from the knowledge graph\")\n",
    "\n",
    "# For demonstration, we'll enrich a subset of entities\n",
    "# In a real scenario, you might want to enrich all entities\n",
    "import random\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "# Select persons and organizations preferentially\n",
    "persons = [e for e in entities if e[1] == 'PERSON']\n",
    "organizations = [e for e in entities if e[1] == 'ORG']\n",
    "locations = [e for e in entities if e[1] == 'LOC']\n",
    "\n",
    "# Determine how many of each to select\n",
    "max_entities = 800\n",
    "person_count = min(len(persons), max_entities // 3)\n",
    "location_count = min(len(locations), max_entities // 3)\n",
    "org_count = min(len(organizations), max_entities - person_count - location_count)\n",
    "\n",
    "# Select the entities\n",
    "selected_persons = random.sample(persons, person_count) if person_count > 0 else []\n",
    "selected_locs = random.sample(locations, location_count) if location_count> 0 else []\n",
    "selected_orgs = random.sample(organizations, org_count) if org_count > 0 else []\n",
    "\n",
    "# Combine them\n",
    "selected_entities = selected_persons + selected_locs + selected_orgs\n",
    "\n",
    "print(f\"Selected {len(selected_entities)} entities for enrichment:\")\n",
    "for entity_uri, entity_type in selected_entities[:5]:  # Show first 5\n",
    "    # Get entity label\n",
    "    entity_text, _ = augmenter.get_entity_metadata(entity_uri)\n",
    "    print(f\"- {entity_text} ({entity_type})\")\n",
    "if len(selected_entities) > 5:\n",
    "    print(f\"  ... and {len(selected_entities) - 5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrich selected entities\n",
    "print(\"Enriching entities with DBpedia and Wikidata knowledge...\")\n",
    "for i, (entity_uri, entity_type) in enumerate(selected_entities):\n",
    "    print(f\"\\nProcessing entity {i+1}/{len(selected_entities)}: {entity_uri}\")\n",
    "    added = augmenter.enrich_entity(\n",
    "        entity_uri,\n",
    "        use_dbpedia=True,\n",
    "        use_wikidata=False, # Doesn't work for some reason...\n",
    "        connection_depth=1\n",
    "    )\n",
    "    print(f\"Added {added} triples for this entity\")\n",
    "\n",
    "# Save the augmented knowledge graph\n",
    "augmenter.save_graph(augmented_kg_path)\n",
    "\n",
    "# Get statistics about the enrichment\n",
    "stats = augmenter.compute_enrichment_stats()\n",
    "\n",
    "print(f\"\\nAugmentation complete!\")\n",
    "print(f\"Original triples: {len(original_graph)}\")\n",
    "print(f\"Added triples: {stats['total_triples'] - len(original_graph)}\")\n",
    "print(f\"Total triples in augmented graph: {stats['total_triples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, RDF, RDFS\n",
    "from src.knowledge_graph.builder import KnowledgeGraphBuilder\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "# Path to your augmented knowledge graph file\n",
    "augmented_kg_path = './output/data/knowledge_graph_augmented.ttl'\n",
    "\n",
    "# Load the graph\n",
    "g = Graph()\n",
    "g.parse(augmented_kg_path, format='turtle')\n",
    "\n",
    "# Extract entity types and labels\n",
    "entity_types = {}\n",
    "entity_labels = {}\n",
    "\n",
    "for s, p, o in g.triples((None, RDF.type, None)):\n",
    "    entity_types[str(s)] = str(o).split('/')[-1]\n",
    "\n",
    "for s, p, o in g.triples((None, RDFS.label, None)):\n",
    "    entity_labels[str(s)] = str(o)\n",
    "\n",
    "# Create a fresh NetworkX graph\n",
    "nx_graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes with proper attributes\n",
    "for entity, entity_type in entity_types.items():\n",
    "    label = entity_labels.get(entity, entity.split('/')[-1])\n",
    "    nx_graph.add_node(entity, label=label, title=f\"{label} ({entity_type})\", type=entity_type)\n",
    "\n",
    "# Add edges without RDF.type predicates\n",
    "for s, p, o in g:\n",
    "    if p != RDF.type and p != RDFS.label:  # Skip type and label triples\n",
    "        s_str, p_str, o_str = str(s), str(p), str(o)\n",
    "        \n",
    "        # Only add edges between nodes that exist\n",
    "        if s_str in nx_graph and o_str in nx_graph:\n",
    "            # Get a shorter predicate name for display\n",
    "            p_label = p_str.split('/')[-1]\n",
    "            # Add edge with label and title\n",
    "            nx_graph.add_edge(s_str, o_str, label=p_label, title=p_label)\n",
    "\n",
    "# Create visualization with custom settings\n",
    "output_path = './output/visualization/knowledge_graph_augmented.html'\n",
    "\n",
    "# Create a PyVis network\n",
    "net = Network(notebook=True, directed=True, height=\"750px\", width=\"100%\")\n",
    "net.from_nx(nx_graph)\n",
    "\n",
    "# Map entity types to colors\n",
    "color_map = {\n",
    "    'PERSON': '#a8e6cf',\n",
    "    'ORG': '#ff8b94',\n",
    "    'GPE': '#ffd3b6',\n",
    "    'LOC': '#dcedc1',\n",
    "    'DATE': '#f9f9f9',\n",
    "    'MISC': '#d4a5a5'\n",
    "}\n",
    "\n",
    "# Update node colors and sizes\n",
    "for node in net.nodes:\n",
    "    # Set color based on type\n",
    "    node_type = node.get('type', 'Unknown')\n",
    "    node['color'] = color_map.get(node_type, '#b3b3cc')\n",
    "    \n",
    "    # Set size based on node importance\n",
    "    node['size'] = 15\n",
    "    \n",
    "    # Use shortened labels for display\n",
    "    if 'label' in node:\n",
    "        if len(node['label']) > 20:\n",
    "            node['label'] = node['label'][:17] + '...'\n",
    "\n",
    "# Save the visualization\n",
    "net.save_graph(output_path)\n",
    "print(f\"Visualization created at: {output_path}\")\n",
    "print(f\"Nodes: {len(nx_graph.nodes)}, Edges: {len(nx_graph.edges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Graph Embedding\n",
    "\n",
    "Now, let's create embeddings for our augmented knowledge graph using different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing: Removing Isolated Nodes\")\n",
    "\n",
    "from rdflib import Graph, RDF, RDFS, URIRef\n",
    "import networkx as nx\n",
    "\n",
    "# Load the augmented knowledge graph\n",
    "kg_graph = Graph()\n",
    "kg_graph.parse(augmented_kg_path, format='turtle')\n",
    "\n",
    "# Count original triples and entities\n",
    "original_triple_count = len(kg_graph)\n",
    "original_entity_count = len({s for s, p, o in kg_graph.triples((None, RDF.type, None))})\n",
    "\n",
    "print(f\"Original knowledge graph: {original_triple_count} triples, {original_entity_count} entities\")\n",
    "\n",
    "# Create a NetworkX graph to find connected components\n",
    "nx_graph = nx.Graph()\n",
    "\n",
    "# Add edges for all non-RDF.type and non-RDFS.label predicates\n",
    "# These predicates represent actual relationships between entities\n",
    "for s, p, o in kg_graph:\n",
    "    if p != RDF.type and p != RDFS.label and isinstance(s, URIRef) and isinstance(o, URIRef):\n",
    "        nx_graph.add_edge(str(s), str(o))\n",
    "\n",
    "# Get degree of each node\n",
    "node_degrees = dict(nx_graph.degree())\n",
    "isolated_nodes = [node for node, degree in node_degrees.items() if degree == 0]\n",
    "connected_nodes = [node for node, degree in node_degrees.items() if degree > 0]\n",
    "\n",
    "print(f\"Found {len(isolated_nodes)} isolated nodes and {len(connected_nodes)} connected nodes\")\n",
    "\n",
    "# Create a new graph with only connected nodes\n",
    "filtered_graph = Graph()\n",
    "\n",
    "# Copy namespace bindings\n",
    "for prefix, namespace in kg_graph.namespaces():\n",
    "    filtered_graph.bind(prefix, namespace)\n",
    "\n",
    "# Add triples where both subject and object are in connected nodes\n",
    "# or one is in connected nodes and the other is a literal\n",
    "for s, p, o in kg_graph:\n",
    "    s_str = str(s)\n",
    "    o_str = str(o)\n",
    "    \n",
    "    # Always include type and label information for connected nodes\n",
    "    if (p == RDF.type or p == RDFS.label) and s_str in connected_nodes:\n",
    "        filtered_graph.add((s, p, o))\n",
    "    # Include relationship triples between connected nodes\n",
    "    elif isinstance(s, URIRef) and isinstance(o, URIRef) and s_str in connected_nodes and o_str in connected_nodes:\n",
    "        filtered_graph.add((s, p, o))\n",
    "    # Include literal properties of connected nodes\n",
    "    elif s_str in connected_nodes and not isinstance(o, URIRef):\n",
    "        filtered_graph.add((s, p, o))\n",
    "\n",
    "# Save the filtered graph\n",
    "filtered_kg_path = './output/data/knowledge_graph_filtered.ttl'\n",
    "filtered_graph.serialize(destination=filtered_kg_path, format='turtle')\n",
    "\n",
    "# Count filtered triples and entities\n",
    "filtered_triple_count = len(filtered_graph)\n",
    "filtered_entity_count = len({s for s, p, o in filtered_graph.triples((None, RDF.type, None))})\n",
    "\n",
    "print(f\"Filtered knowledge graph: {filtered_triple_count} triples, {filtered_entity_count} entities\")\n",
    "print(f\"Removed {original_triple_count - filtered_triple_count} triples and {original_entity_count - filtered_entity_count} entities\")\n",
    "print(f\"Saved filtered knowledge graph to {filtered_kg_path}\")\n",
    "\n",
    "# Update the path to use the filtered graph for embeddings\n",
    "augmented_kg_path = filtered_kg_path\n",
    "\n",
    "\n",
    "# Create visualization with custom settings\n",
    "output_path = './output/visualization/knowledge_graph_filtered.html'\n",
    "\n",
    "# Extract entity types and labels from the filtered graph\n",
    "entity_types = {}\n",
    "for s, p, o in filtered_graph.triples((None, RDF.type, None)):\n",
    "   entity_types[str(s)] = str(o).split('/')[-1]\n",
    "\n",
    "entity_labels = {}\n",
    "for s, p, o in filtered_graph.triples((None, RDFS.label, None)):\n",
    "   entity_labels[str(s)] = str(o)\n",
    "\n",
    "# Create a fresh NetworkX graph with proper node attributes\n",
    "vis_graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes with type and label information\n",
    "for entity, entity_type in entity_types.items():\n",
    "   label = entity_labels.get(entity, entity.split('/')[-1])\n",
    "   vis_graph.add_node(entity, label=label, title=f\"{label} ({entity_type})\", type=entity_type)\n",
    "\n",
    "# Add edges with proper labels\n",
    "for s, p, o in filtered_graph:\n",
    "   if p != RDF.type and p != RDFS.label and isinstance(s, URIRef) and isinstance(o, URIRef):\n",
    "       s_str = str(s)\n",
    "       o_str = str(o)\n",
    "       \n",
    "       # Get a readable predicate name\n",
    "       p_label = str(p).split('/')[-1]\n",
    "       \n",
    "       # Only add if both nodes exist\n",
    "       if s_str in vis_graph and o_str in vis_graph:\n",
    "           vis_graph.add_edge(s_str, o_str, label=p_label, title=p_label)\n",
    "\n",
    "# Create a PyVis network\n",
    "net = Network(notebook=True, directed=True, height=\"750px\", width=\"100%\")\n",
    "net.from_nx(vis_graph)\n",
    "\n",
    "# Map entity types to colors\n",
    "color_map = {\n",
    "   'PERSON': '#a8e6cf',\n",
    "   'ORG': '#ff8b94',\n",
    "   'GPE': '#ffd3b6',\n",
    "   'LOC': '#dcedc1',\n",
    "   'DATE': '#f9f9f9',\n",
    "   'MISC': '#d4a5a5',\n",
    "   'DBPEDIA': '#b19cd9',\n",
    "   'WIKIDATA': '#ffd700'\n",
    "}\n",
    "\n",
    "# Update node colors and sizes\n",
    "for node in net.nodes:\n",
    "   # Set color based on type\n",
    "   node_type = node.get('type', 'Unknown')\n",
    "   node['color'] = color_map.get(node_type, '#b3b3cc')\n",
    "   \n",
    "   # Set size based on node importance\n",
    "   node['size'] = 15\n",
    "   \n",
    "   # Use shortened labels for display\n",
    "   if 'label' in node:\n",
    "       if len(node['label']) > 20:\n",
    "           node['label'] = node['label'][:17] + '...'\n",
    "\n",
    "# Configure physics for better layout\n",
    "net.set_options(\"\"\"\n",
    "{\n",
    " \"physics\": {\n",
    "   \"barnesHut\": {\n",
    "     \"gravitationalConstant\": -2000,\n",
    "     \"centralGravity\": 0.1,\n",
    "     \"springLength\": 95,\n",
    "     \"springConstant\": 0.04\n",
    "   },\n",
    "   \"maxVelocity\": 50,\n",
    "   \"minVelocity\": 0.75,\n",
    "   \"solver\": \"barnesHut\",\n",
    "   \"timestep\": 0.5\n",
    " },\n",
    " \"edges\": {\n",
    "   \"color\": {\n",
    "     \"inherit\": true\n",
    "   },\n",
    "   \"smooth\": {\n",
    "     \"enabled\": true,\n",
    "     \"type\": \"dynamic\"\n",
    "   },\n",
    "   \"arrows\": {\n",
    "     \"to\": {\n",
    "       \"enabled\": true,\n",
    "       \"scaleFactor\": 0.5\n",
    "     }\n",
    "   },\n",
    "   \"font\": {\n",
    "     \"size\": 8\n",
    "   }\n",
    " },\n",
    " \"nodes\": {\n",
    "   \"font\": {\n",
    "     \"size\": 12\n",
    "   }\n",
    " }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Save the visualization\n",
    "net.save_graph(output_path)\n",
    "print(f\"Visualization created at: {output_path}\")\n",
    "print(f\"Nodes: {len(vis_graph.nodes)}, Edges: {len(vis_graph.edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedder\n",
    "embedder = KnowledgeGraphEmbedder(namespace=\"http://example.org/graphify/\")\n",
    "\n",
    "# Load the augmented knowledge graph\n",
    "embedder.load_from_rdf(augmented_kg_path)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "training, validation, testing = embedder.split_dataset(train_ratio=0.8, validation_ratio=0.1, test_ratio=0.1)\n",
    "\n",
    "print(f\"Split dataset:\")\n",
    "print(f\"- Training: {training.num_triples} triples\")\n",
    "print(f\"- Validation: {validation.num_triples} triples\")\n",
    "print(f\"- Testing: {testing.num_triples} triples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train embeddings with TransE model\n",
    "print(\"Training TransE model...\")\n",
    "transe_result = embedder.train_embedding_model(\n",
    "    model_name='TransE',\n",
    "    training=training,\n",
    "    validation=validation,\n",
    "    testing=testing,\n",
    "    epochs=100,\n",
    "    embedding_dim=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    num_negs_per_pos=10,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train embeddings with DistMult model\n",
    "print(\"Training DistMult model...\")\n",
    "distmult_result = embedder.train_embedding_model(\n",
    "    model_name='DistMult',\n",
    "    training=training,\n",
    "    validation=validation,\n",
    "    testing=testing,\n",
    "    epochs=100,\n",
    "    embedding_dim=50,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    num_negs_per_pos=10,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "Now, let's evaluate the performance of our embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = embedder.compare_models(['TransE', 'DistMult'])\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure and subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "# Plot Rank-Based Metrics\n",
    "comparison_df[['mean_rank']].plot(kind='bar', ax=axes[0], legend=False, color=['#1f77b4'])\n",
    "axes[0].set_title('Mean Rank Comparison')\n",
    "axes[0].set_ylabel('Mean Rank')\n",
    "axes[0].set_xlabel('Model')\n",
    "\n",
    "# Plot Hits@K Metrics\n",
    "comparison_df[['hits_at_1', 'hits_at_3', 'hits_at_10']].plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Hits@K Comparison')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_xlabel('Model')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the figure\n",
    "plt.savefig('output/visualization/embedding_model_comparison.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entity Similarity Analysis\n",
    "\n",
    "Let's find similar entities based on our learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few sample entities\n",
    "sample_entity_uris = []\n",
    "sample_entity_texts = []\n",
    "\n",
    "# First, get all entities with their labels\n",
    "entity_labels = {}\n",
    "from rdflib.namespace import RDFS\n",
    "for s, p, o in augmenter.graph.triples((None, RDFS.label, None)):\n",
    "    entity_labels[str(s)] = str(o)\n",
    "\n",
    "# Choose a few sample entities of different types\n",
    "person_entities = []\n",
    "org_entities = []\n",
    "loc_entities = []\n",
    "\n",
    "for entity_uri, entity_type in entities:\n",
    "    if entity_uri in entity_labels:\n",
    "        if entity_type == 'PERSON' and len(person_entities) < 3:\n",
    "            person_entities.append((entity_uri, entity_labels[entity_uri]))\n",
    "        elif entity_type == 'ORG' and len(org_entities) < 3:\n",
    "            org_entities.append((entity_uri, entity_labels[entity_uri]))\n",
    "        elif entity_type in ('GPE', 'LOC') and len(loc_entities) < 3:\n",
    "            loc_entities.append((entity_uri, entity_labels[entity_uri]))\n",
    "    \n",
    "    if len(person_entities) >= 3 and len(org_entities) >= 3 and len(loc_entities) >= 3:\n",
    "        break\n",
    "\n",
    "# Combine all sample entities\n",
    "sample_entities = person_entities + org_entities + loc_entities\n",
    "\n",
    "# Print sample entities\n",
    "print(\"Sample entities for similarity analysis:\")\n",
    "for entity_uri, entity_text in sample_entities:\n",
    "    print(f\"- {entity_text} ({entity_uri})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar entities using TransE embeddings\n",
    "print(\"Similar entities using TransE embeddings:\")\n",
    "for entity_uri, entity_text in sample_entities:\n",
    "    print(f\"\\nEntities similar to '{entity_text}':\")\n",
    "    similar_entities = embedder.find_similar_entities(entity_uri, 'TransE', top_k=5)\n",
    "    for similar_uri, similarity in similar_entities:\n",
    "        print(f\"- {similar_uri} (Similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar entities using DistMult embeddings\n",
    "print(\"Similar entities using DistMult embeddings:\")\n",
    "for entity_uri, entity_text in sample_entities[:3]:  # Just show a few for brevity\n",
    "    print(f\"\\nEntities similar to '{entity_text}':\")\n",
    "    similar_entities = embedder.find_similar_entities(entity_uri, 'DistMult', top_k=5)\n",
    "    for similar_uri, similarity in similar_entities:\n",
    "        print(f\"- {similar_uri} (Similarity: {similarity:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Link Prediction\n",
    "\n",
    "Now, let's test the link prediction capabilities of our embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some relations from the knowledge graph\n",
    "relations = set()\n",
    "for s, p, o in augmenter.graph:\n",
    "    if p != RDF.type and p != RDFS.label:\n",
    "        relations.add(str(p))\n",
    "\n",
    "# Choose a few sample relations\n",
    "sample_relations = list(relations)[:5] if len(relations) >= 5 else list(relations)\n",
    "\n",
    "print(\"Sample relations for link prediction:\")\n",
    "for relation in sample_relations:\n",
    "    print(f\"- {relation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find entity pairs for chosen relations\n",
    "relation_triples = []\n",
    "\n",
    "for relation in sample_relations:\n",
    "    # Find triples with this relation\n",
    "    relation_uri = relation\n",
    "    for s, p, o in augmenter.graph.triples((None, URIRef(relation_uri), None)):\n",
    "        # Check if subject and object entities exist in embedder\n",
    "        if str(s) in embedder.entity_to_id and str(o) in embedder.entity_to_id:\n",
    "            # Get entity labels\n",
    "            s_label = entity_labels.get(str(s), str(s).split('/')[-1])\n",
    "            o_label = entity_labels.get(str(o), str(o).split('/')[-1])\n",
    "            \n",
    "            relation_triples.append((str(s), s_label, relation_uri, str(o), o_label))\n",
    "            break  # Just take one example per relation\n",
    "\n",
    "print(\"Sample triples for link prediction:\")\n",
    "for s, s_label, p, o, o_label in relation_triples:\n",
    "    p_short = p.split('/')[-1]\n",
    "    print(f\"- {s_label} --[{p_short}]--> {o_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict tail entities\n",
    "print(\"Predicting tail entities with TransE:\")\n",
    "for s, s_label, p, o, o_label in relation_triples:\n",
    "    p_short = p.split('/')[-1]\n",
    "    print(f\"\\nGiven head '{s_label}' and relation '{p_short}', predicted tails:\")\n",
    "    predicted_tails = embedder.predict_tail_entities(s, p, 'TransE', top_k=5)\n",
    "    for tail_uri, score in predicted_tails:\n",
    "        print(f\"- {tail_uri} (Score: {score:.4f})\")\n",
    "    \n",
    "    # Check if the actual tail is in the predictions\n",
    "    predicted_uris = [uri for uri, _ in predicted_tails]\n",
    "    if o in predicted_uris:\n",
    "        print(f\"✓ Actual tail '{o_label}' is in the predictions at rank {predicted_uris.index(o) + 1}\")\n",
    "    else:\n",
    "        print(f\"✗ Actual tail '{o_label}' is not in the top 5 predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict head entities\n",
    "print(\"Predicting head entities with DistMult:\")\n",
    "for s, s_label, p, o, o_label in relation_triples:\n",
    "    p_short = p.split('/')[-1]\n",
    "    print(f\"\\nGiven relation '{p_short}' and tail '{o_label}', predicted heads:\")\n",
    "    predicted_heads = embedder.predict_head_entities(p, o, 'DistMult', top_k=5)\n",
    "    for head_uri, score in predicted_heads:\n",
    "        print(f\"- {head_uri} (Score: {score:.4f})\")\n",
    "    \n",
    "    # Check if the actual head is in the predictions\n",
    "    predicted_uris = [uri for uri, _ in predicted_heads]\n",
    "    if s in predicted_uris:\n",
    "        print(f\"✓ Actual head '{s_label}' is in the predictions at rank {predicted_uris.index(s) + 1}\")\n",
    "    else:\n",
    "        print(f\"✗ Actual head '{s_label}' is not in the top 5 predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization of Embeddings\n",
    "\n",
    "Let's visualize the entity embeddings to see how they cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group entities by type for visualization\n",
    "entity_type_triples = []\n",
    "for s, p, o in augmenter.graph.triples((None, RDF.type, None)):\n",
    "    entity_type_triples.append((str(s), 'rdf:type', str(o)))\n",
    "\n",
    "# Create entity type mappings for visualization\n",
    "entity_types = embedder.group_entities_by_type(entity_type_triples)\n",
    "\n",
    "# Visualize TransE embeddings\n",
    "embedder.visualize_embeddings(\n",
    "    model_name='TransE',\n",
    "    entity_types=entity_types,\n",
    "    sample_size=1000,  # Limit for better visualization\n",
    "    figsize=(12, 10),\n",
    "    output_path='output/visualization/transe_embeddings.png',\n",
    "    title='TransE Entity Embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DistMult embeddings\n",
    "embedder.visualize_embeddings(\n",
    "    model_name='DistMult',\n",
    "    entity_types=entity_types,\n",
    "    sample_size=1000,  # Limit for better visualization\n",
    "    figsize=(12, 10),\n",
    "    output_path='output/visualization/distmult_embeddings.png',\n",
    "    title='DistMult Entity Embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Models and Embeddings\n",
    "\n",
    "Finally, let's save our models and embeddings for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "models_dir = 'output/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "for model_name in ['TransE', 'DistMult']:\n",
    "    embedder.save_model(model_name, models_dir)\n",
    "\n",
    "# Save entity and relation embeddings as NumPy arrays\n",
    "embeddings_dir = 'output/embeddings'\n",
    "os.makedirs(embeddings_dir, exist_ok=True)\n",
    "\n",
    "for model_name in ['TransE', 'DistMult']:\n",
    "    # Get embeddings\n",
    "    entity_embeddings = embedder.get_entity_embeddings(model_name)\n",
    "    relation_embeddings = embedder.get_relation_embeddings(model_name)\n",
    "    \n",
    "    # Save embeddings\n",
    "    np.save(os.path.join(embeddings_dir, f\"{model_name}_entity_embeddings.npy\"), entity_embeddings)\n",
    "    np.save(os.path.join(embeddings_dir, f\"{model_name}_relation_embeddings.npy\"), relation_embeddings)\n",
    "    \n",
    "    print(f\"Saved {model_name} embeddings:\")\n",
    "    print(f\"- Entity embeddings: {entity_embeddings.shape}\")\n",
    "    print(f\"- Relation embeddings: {relation_embeddings.shape}\")\n",
    "\n",
    "# Save entity and relation mappings as JSON\n",
    "mappings = {\n",
    "    'entity_to_id': {k: v for k, v in embedder.entity_to_id.items()},\n",
    "    'relation_to_id': {k: v for k, v in embedder.relation_to_id.items()},\n",
    "    'id_to_entity': {str(k): v for k, v in embedder.id_to_entity.items()},\n",
    "    'id_to_relation': {str(k): v for k, v in embedder.id_to_relation.items()}\n",
    "}\n",
    "\n",
    "with open(os.path.join(embeddings_dir, 'mappings.json'), 'w') as f:\n",
    "    json.dump(mappings, f, indent=2)\n",
    "\n",
    "print(\"Saved entity and relation mappings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
